{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5082707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "General purpose functions for the br_cenipa project\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import dotenv\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Wehscraping option libs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# API option libs\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Internals\n",
    "from constants import constants\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(constants.ROOT_DIR.value, 'tmp',f\"logging{datetime.now().strftime('%Y-%m-%d-%H%M')}.txt\"))\n",
    "\n",
    "# logger.add(file_handler,   \n",
    "#            level='INFO')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def set_driver():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\") \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    prefs = {\"download.default_directory\" : constants.INPUT_DIR_PATH.value}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def download_table_to_csv(table_name, table_url, table_path=constants.INPUT_DIR_PATH.value):\n",
    "    \"\"\"\n",
    "        Downloads a table from the CENIPA dataset and saves it as a CSV file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(table_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if os.path.exists(table_path) is False:\n",
    "            os.makedirs(table_path)\n",
    "\n",
    "        file_path = os.path.join(table_path, f\"{table_name}.csv\")\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded {table_name} to {file_path}\")\n",
    "        print(f\"Downloaded {table_name} to {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "    \n",
    "def correct_csv_encoding():\n",
    "    \"\"\"\n",
    "        Corrects the encoding of CSV files in the input directory from 'latin1' to 'utf-8'.\n",
    "    \"\"\"\n",
    "    logging.info(\"Correcting CSV file encodings from 'latin1' to 'utf-8'...\")\n",
    "    print(\"Correcting CSV file encodings from 'latin1' to 'utf-8'...\")\n",
    "    for  file_name in os.listdir(constants.INPUT_DIR_PATH.value):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(constants.INPUT_DIR_PATH.value, file_name)\n",
    "            pd.read_csv(file_path, sep=\";\", encoding=\"latin1\")\\\n",
    "            .to_csv(file_path, sep=\";\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "def show_uniques(df, columns):\n",
    "    \"\"\"\n",
    "        Displays unique values for specified columns in the dataset.\n",
    "    \"\"\"\n",
    "    logging.info('-----------------------------------------------------------------------------')\n",
    "    logging.info(\"Showing unique values for specified columns...\")\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "    print(\"Showing unique values for specified columns...\")\n",
    "    for col in columns:\n",
    "        if not col.startswith('id_'):\n",
    "            unique_values = df[col].unique()\n",
    "            logging.info(f\"Unique values in {col}: {unique_values}\")\n",
    "            print(f\"Unique values in {col}: {unique_values}\")\n",
    "\n",
    "## Inconsistencies\n",
    "def check_inconsistences(dataframe:pd.DataFrame):\n",
    "    # Check for unique values in the 'id_ocorrencia' column\n",
    "    if not dataframe['id_ocorrencia'].is_unique:\n",
    "        logging.warning(\"The 'id_ocorrencia' column should have unique values.\")\n",
    "        print(\"The 'id_ocorrencia' column should have unique values.\")\n",
    "        logging.warning(dataframe.loc[dataframe['id_ocorrencia'].duplicated(),['id_ocorrencia']])\n",
    "        print(dataframe.loc[dataframe['id_ocorrencia'].duplicated(),['id_ocorrencia']])\n",
    "    if not dataframe[dataframe.duplicated()].empty:\n",
    "        logging.warning(\"The dataframe has duplicated rows:\")\n",
    "        print(\"The dataframe has duplicated rows:\")\n",
    "        logging.warning(dataframe[dataframe.duplicated()])\n",
    "        print(dataframe[dataframe.duplicated()])\n",
    "    # Check for missing values in the columns\n",
    "    for col in dataframe.columns:\n",
    "        if dataframe[col].isnull().any():\n",
    "            logging.warning(f\"Column '{col}' has missing values.\")\n",
    "            print(f\"Column '{col}' has missing values.\")\n",
    "        if col.startswith('id'):\n",
    "            # Check for unique values in the 'id_relatorio' column\n",
    "            if dataframe[col].is_unique:\n",
    "                pass\n",
    "            else:\n",
    "                logging.warning(f\"The {col} has duplicated values. Shouldn't they be unique?\")\n",
    "    # Check for duplicate rows\n",
    "    if dataframe.duplicated().any():\n",
    "        logging.warning(\"There are duplicate rows in the DataFrame.\")\n",
    "        print(\"There are duplicate rows in the DataFrame.\")\n",
    "        logging.info(dataframe[dataframe.duplicated()])\n",
    "        print(dataframe[dataframe.duplicated()])\n",
    "\n",
    "## Formatting String Columns        \n",
    "# Convert string columns to lowercase with first letter of each word capitalized (except connectors) \n",
    "def format_string(dataframe:pd.DataFrame, string_columns:List[str]):\n",
    "    try:\n",
    "        for col in string_columns:\n",
    "            try:\n",
    "\n",
    "                dataframe[col] = dataframe[col]\\\n",
    "                    .astype(str)\\\n",
    "                    .str.strip()\\\n",
    "                    .str.replace(r'\\*|nan|Nan', '', regex=True)\\\n",
    "                    .str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "                if not col.startswith('id'):\n",
    "                    dataframe[col] = dataframe[col].str.lower()\n",
    "            \n",
    "                if col.startswith('nome'):\n",
    "                    dataframe[col] = dataframe[col].str.title()\n",
    "                    dataframe[col] = dataframe[col].str.replace(\n",
    "                    r'\\b(De|Da|Do|Das|Dos|E|D\\')\\b', \n",
    "                    lambda x: x.group(0).lower(), \n",
    "                    regex=True)\n",
    "            \n",
    "                if col.startswith('sigla'):\n",
    "                    dataframe[col] = dataframe[col].str.upper()\n",
    "\n",
    "                dataframe[col] = dataframe[col].fillna('')\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unable to cast column {col} to string type due to: {e}\")\n",
    "                print(f\"Unable to cast column {col} to string type due to: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to cast columns to string type due to: {e}\\nStopped at {col} column.\")\n",
    "        print(f\"Unable to cast columns to string type due to: {e}\\nStopped at {col} column.\")\n",
    "\n",
    "## Formatting float columns\n",
    "def transform_lat_long(value:str):\n",
    "    extraction = re.findall(r'-?[\\d\\.]+', value)\n",
    "    if extraction:\n",
    "        value_match = re.match(r'(^-?\\d+)([\\.\\d]+)', extraction[0])\n",
    "        if value_match:\n",
    "            new_value = f\"\"\"{\n",
    "                value_match.groups()[0][:-1]\n",
    "                }.{\n",
    "                    value_match.groups()[0][-1]\n",
    "                    }{\n",
    "                        str(value_match.groups()[1]).replace('.','')\n",
    "                        }\"\"\"\n",
    "        else:\n",
    "            new_value = extraction[0]\n",
    "        return new_value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def format_floats(dataframe:pd.DataFrame, float_columns:List[str]):\n",
    "    try:\n",
    "        for col in float_columns:\n",
    "            dataframe[col] = dataframe[col]\\\n",
    "                .astype(str)\\\n",
    "                .str.strip() \n",
    "              \n",
    "            dataframe[col] = dataframe[col].str.replace(r'NaN|nan', '0', regex=True) \n",
    "            dataframe[col] = dataframe[col].str.replace(r'\\s+', '', regex=True) \n",
    "        \n",
    "            dataframe[col] = dataframe[col].str.replace(',','.')\n",
    "        try:\n",
    "            dataframe[col] = dataframe[col].astype(float)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unable to cast column {col} to float type due to: {e}\")\n",
    "            print(f\"Unable to cast column {col} to float type due to: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to cast columns to float type due to: {e}\\nStopped at {col} column.\")\n",
    "        print(f\"Unable to cast columns to float type due to: {e}\\nStopped at {col} column.\")\n",
    "\n",
    "## Formatting Date Columns\n",
    "# Convert date columns to datetime format\n",
    "def format_date(dataframe:pd.DataFrame, date_columns:List[str]):\n",
    "    try:\n",
    "        for col in date_columns:\n",
    "            try:\n",
    "                dataframe[col] = dataframe[col]\\\n",
    "                    .astype(str)\\\n",
    "                    .str.strip()\\\n",
    "                    .fillna('')\n",
    "                formats = [\"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y-%m-%d\", \"%Y/%m/%d\"]\n",
    "                i=0\n",
    "                while i<len(formats):\n",
    "                    try:\n",
    "                        dataframe[col] = pd.to_datetime(dataframe[col],\n",
    "                                                        format = formats[i])\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        i+=1\n",
    "\n",
    "                dataframe[col] = pd.to_datetime(dataframe[col], errors='coerce')\n",
    "                dataframe[col] = dataframe[col].dt.strftime('%Y-%m-%d')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unable to cast column {col} to date type due to: {e}\")\n",
    "                print(f\"Unable to cast column {col} to date type due to: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to cast columns to date type due to: {e}\\nStopped at {col} column.\")\n",
    "        print(f\"Unable to cast columns to date type due to: {e}\\nStopped at {col} column.\")\n",
    "\n",
    "## Formatting Timestamp Columns\n",
    "def format_time(dataframe:pd.DataFrame, timestamp_columns:List[str]):\n",
    "    try:\n",
    "        for col in timestamp_columns:\n",
    "            try:\n",
    "                dataframe[col] = dataframe[col]\\\n",
    "                    .astype(str)\\\n",
    "                    .str.strip()\\\n",
    "                    .fillna('')\n",
    "                \n",
    "                dataframe[col] = pd.to_datetime(dataframe[col],\n",
    "                                                format=\"%H:%M:%S\",\n",
    "                                                errors='coerce')                                                    \n",
    "                dataframe[col] = dataframe[col].dt.strftime('%H:%M:%S')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unable to cast column {col} to timestamp type due to: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to cast columns to timestamp type due to: {e}\\nStopped at {col} column.\")\n",
    "        print(f\"Unable to cast columns to timestamp type due to: {e}\\nStopped at {col} column.\")\n",
    "\n",
    "## Formatting Boolean Columns\n",
    "# Convert boolean columns to boolean type\n",
    "def format_bools(dataframe:pd.DataFrame, bool_columns:List[str]):\n",
    "    try:\n",
    "        for col in bool_columns:\n",
    "            try:\n",
    "                dataframe[col] = dataframe[col]\\\n",
    "                    .astype(str)\\\n",
    "                    .str.strip()\\\n",
    "                    .str.lower()\\\n",
    "                    .fillna('')\n",
    "                dataframe.loc[dataframe[col]=='sim',[col]] = 'True'     \n",
    "                dataframe.loc[dataframe[col]=='não',[col]] = 'False'\n",
    "                dataframe[col] = dataframe[col].astype(bool)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unable to cast column {col} to bool type due to: {e}\")\n",
    "                print(f\"Unable to cast column {col} to bool type due to: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unable to cast columns to bool type due to: {e}\\nStopped at {col} column.\")\n",
    "        print(f\"Unable to cast columns to bool type due to: {e}\\nStopped at {col} column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f77fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Tasks for br_cenipa\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from constants import *\n",
    "\n",
    "# Fact table\n",
    "\n",
    "def load_fact_table()->pd.DataFrame:\n",
    "    df_fact_table = pd.read_csv(\n",
    "    os.path.join(constants.INPUT_DIR_PATH.value, \"ocorrencia.csv\"),\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\")\n",
    "    \n",
    "    return df_fact_table\n",
    "\n",
    "\n",
    "def check_fact_table(df_fact_table:pd.DataFrame)->pd.DataFrame:\n",
    "    logging.info(f\"Checking fact table code columns for inconsistencies...\")\n",
    "    # print(f\"Checking fact table code columns for inconsistencies...\")\n",
    "    columns_code = [\n",
    "        'codigo_ocorrencia', \n",
    "        'codigo_ocorrencia1',\n",
    "        'codigo_ocorrencia2',\n",
    "        'codigo_ocorrencia3',\n",
    "        'codigo_ocorrencia4']\n",
    "    df_null = pd.DataFrame([])\n",
    "    df_null = df_fact_table[df_fact_table[columns_code].isnull().any(axis=1)]\n",
    "    if not df_null.empty:\n",
    "        logging.info(f\"Any row with one or more nulls: {df_null}\")\n",
    "        print(f\"Any row with one or more nulls: {df_null}\")\n",
    "        df_null = pd.DataFrame([])\n",
    "    df_null = df_fact_table[df_fact_table[columns_code].isnull().all(axis=1)]\n",
    "    if not df_null.empty:\n",
    "        logging.info(f\"Any null row: {df_null}\")\n",
    "        print(f\"Any null row: {df_null}\")\n",
    "\n",
    "    df_fact_table[(df_fact_table['codigo_ocorrencia'] == df_fact_table['codigo_ocorrencia1'])&\\\n",
    "               (df_fact_table['codigo_ocorrencia'] == df_fact_table['codigo_ocorrencia2'])&\\\n",
    "               (df_fact_table['codigo_ocorrencia'] == df_fact_table['codigo_ocorrencia3'])&\\\n",
    "               (df_fact_table['codigo_ocorrencia'] == df_fact_table['codigo_ocorrencia4'])]\n",
    "    columns_code.remove('codigo_ocorrencia')\n",
    "\n",
    "    # Remove columns with codes that are not unique\n",
    "    df_fact_table_modif = df_fact_table.drop(columns=columns_code)\\\n",
    "        .rename(columns=constants.RENAME_MAPPING.value).copy()\n",
    "\n",
    "    check_inconsistences(df_fact_table_modif)\n",
    "    return df_fact_table_modif\n",
    "\n",
    "# Type casting\n",
    "\n",
    "def type_cast_fact_table(df_fact_table_modif:pd.DataFrame):\n",
    "    df_fact_cast = df_fact_table_modif.copy()\n",
    "    for col in constants.FLOAT_COLUMNS.value:\n",
    "        df_fact_cast[col] = df_fact_cast[col]\\\n",
    "        .astype(str)\\\n",
    "        .str.replace(r'\\*+', '0', regex=True)\\\n",
    "        .replace(r'°', '', regex=True)\\\n",
    "        .apply(transform_lat_long)\n",
    "    format_floats(df_fact_cast,\n",
    "              constants.FLOAT_COLUMNS.value)\n",
    "    format_string(df_fact_cast, constants.STRING_COLUMNS.value)\n",
    "    format_date(df_fact_cast, constants.DATE_COLUMNS.value)\n",
    "    format_time(df_fact_cast, constants.TIMESTAMP_COLUMNS.value)\n",
    "    show_uniques(df_fact_cast, constants.BOOL_COLUMNS.value)\n",
    "    format_bools(df_fact_cast, constants.BOOL_COLUMNS.value)\n",
    "    show_uniques(df_fact_cast, constants.BOOL_COLUMNS.value)\n",
    "\n",
    "    logging.info(\"Checking consistency after transformations...\")\n",
    "    check_inconsistences(df_fact_cast)\n",
    "    df_fact_cast.to_csv(os.path.join(constants.OUTPUT_DIR_PATH.value,\"br_cenipa_ocorrencia.csv\"), index=False)\n",
    "\n",
    "\n",
    "## Dimension tables\n",
    "\n",
    "def load_dim_tables():\n",
    "    logging.info(\"Reading dimension tables...\")\n",
    "    print(\"Reading dimension tables...\")\n",
    "    try:\n",
    "        df_tipos = pd.read_csv(\n",
    "        os.path.join(constants.INPUT_DIR_PATH.value, \"ocorrencia_tipo.csv\"),\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\")\n",
    "\n",
    "        df_aeronave = pd.read_csv(\n",
    "        os.path.join(constants.INPUT_DIR_PATH.value, \"aeronave.csv\"),\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\")\n",
    "\n",
    "        df_fator = pd.read_csv(\n",
    "        os.path.join(constants.INPUT_DIR_PATH.value, \"fator_contribuinte.csv\"),\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\")\n",
    "\n",
    "        df_recomendacao = pd.read_csv(\n",
    "        os.path.join(constants.INPUT_DIR_PATH.value, \"recomendacao.csv\"),\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during dimension tables reading: {e}\")\n",
    "        print(f\"Error during dimension tables reading: {e}\")\n",
    "    dim_tables = [df_tipos,df_aeronave,df_fator,df_recomendacao]\n",
    "    return dim_tables\n",
    "\n",
    "\n",
    "# Renaming columns with mappings for each dataframe\n",
    "\n",
    "def renaming_dim_tables(dim_tables:List[pd.DataFrame])->List[pd.DataFrame]:\n",
    "    logging.info(\"Renaming dimension tables...\")\n",
    "    print(\"Renaming dimension tables...\")\n",
    "    try:\n",
    "        df_tipos_modif = dim_tables[0]\\\n",
    "            .rename(columns=constants.TIPO_RENAME_MAPPING.value).copy()\n",
    "        df_aeronave_modif = dim_tables[1]\\\n",
    "            .rename(columns=constants.AERONAVE_RENAME_MAPPING.value).copy()\n",
    "        df_fator_modif = dim_tables[2]\\\n",
    "            .rename(columns=constants.FATOR_RENAME_MAPPING.value).copy()\n",
    "        df_recomendacao_modif = dim_tables[3]\\\n",
    "            .rename(columns=constants.RECOMENDACAO_RENAME_MAPPING.value).copy()\n",
    "        del dim_tables\n",
    "        return [df_tipos_modif,df_aeronave_modif,df_fator_modif,df_recomendacao_modif]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during dimension tables renaming: {e}\")\n",
    "        print(f\"Error during dimension tables renaming: {e}\")\n",
    "\n",
    "\n",
    "def type_cast_tipo_table(df_tipo_modif:pd.DataFrame):\n",
    "    if df_tipo_modif is not None:\n",
    "        try:\n",
    "            # Type casting\n",
    "            df_tipo_cast = df_tipo_modif.copy()\n",
    "            TIPO_STRING_COLUMNS = list(df_tipo_cast.columns.values)\n",
    "            TIPO_STRING_COLUMNS.remove('id_ocorrencia')\n",
    "            format_string(df_tipo_cast, TIPO_STRING_COLUMNS)\n",
    "            logging.info(\"Checking consistency after transformations...\")\n",
    "            print(\"Checking consistency after transformations...\")\n",
    "            check_inconsistences(df_tipo_cast)\n",
    "            del df_tipo_modif\n",
    "            df_tipo_cast.to_csv(os.path.join(constants.OUTPUT_DIR_PATH.value,\"br_cenipa_tipo_ocorrencia.csv\"), index=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during 'tipo' table type casting: {e}\")\n",
    "            print(f\"Error during 'tipo' table type casting: {e}\")\n",
    "\n",
    "\n",
    "def type_cast_aeronave_table(df_aeronave_modif:pd.DataFrame):\n",
    "    if df_aeronave_modif is not None:\n",
    "        try:\n",
    "            df_aeronave_cast = df_aeronave_modif.copy()\n",
    "            format_string(df_aeronave_cast, constants.AERONAVE_STR_COLUMNS.value)\n",
    "            format_floats(df_aeronave_cast, constants.AERONAVE_INT_COLUMNS.value)\n",
    "            logging.info(\"Checking consistency after transformations...\")\n",
    "            print(\"Checking consistency after transformations...\")\n",
    "            check_inconsistences(df_aeronave_cast)\n",
    "            del df_aeronave_modif\n",
    "            df_aeronave_cast.to_csv(os.path.join(constants.OUTPUT_DIR_PATH.value,\"br_cenipa_aeronave.csv\"), index=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during 'aeronave' table type casting: {e}\")\n",
    "            print(f\"Error during 'aeronave' table type casting: {e}\")\n",
    "\n",
    "def type_cast_fator_table(df_fator_modif:pd.DataFrame):\n",
    "    if df_fator_modif is not None:\n",
    "        try:\n",
    "            df_fator_cast = df_fator_modif.copy()\n",
    "            FATOR_STRING_COLUMNS = list(df_fator_cast.columns.values)\n",
    "            FATOR_STRING_COLUMNS.remove('id_ocorrencia')\n",
    "            format_string(df_fator_cast, FATOR_STRING_COLUMNS)\n",
    "            logging.info(\"Checking consistency after transformations...\")\n",
    "            print(\"Checking consistency after transformations...\")\n",
    "            check_inconsistences(df_fator_cast)\n",
    "            del df_fator_modif\n",
    "            df_fator_cast.to_csv(os.path.join(constants.OUTPUT_DIR_PATH.value,\"br_cenipa_fator_contribuinte.csv\"), index=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during 'fator contribuinte' table type casting: {e}\")\n",
    "            print(f\"Error during 'fator contribuinte' table type casting: {e}\")\n",
    "\n",
    "\n",
    "def type_cast_recom_table(df_recomendacao_modif:pd.DataFrame):\n",
    "    if df_recomendacao_modif is not None:\n",
    "        try:\n",
    "            df_recomendacao_cast = df_recomendacao_modif.copy()\n",
    "            format_string(df_recomendacao_cast, constants.RECOMENDACAO_STR_COLUMNS.value)\n",
    "            format_date(df_recomendacao_cast, constants.RECOMENDACAO_DATE_COLUMNS.value)\n",
    "            logging.info(\"Checking consistency after transformations...\")\n",
    "            print(\"Checking consistency after transformations...\")\n",
    "            check_inconsistences(df_recomendacao_cast)\n",
    "            del df_recomendacao_modif\n",
    "            df_recomendacao_cast.to_csv(os.path.join(constants.OUTPUT_DIR_PATH.value,\"br_cenipa_recomendacao.csv\"), index=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during 'recomendacao' table type casting: {e}\")\n",
    "            print(f\"Error during 'recomendacao' table type casting: {e}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7648bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\n",
    "        os.path.join(\"..\", \"input\", \"aeronave.csv\"),\n",
    "        sep=\";\",\n",
    "        encoding=\"utf-8\")\\\n",
    "        .rename(columns=constants.AERONAVE_RENAME_MAPPING.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bff5f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['id_aeronave'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "993ded9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_aeronave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>PRMYJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PRAKJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PTMXC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>PRWBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>PRCHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13276</th>\n",
       "      <td>PTWKZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13279</th>\n",
       "      <td>PTVCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13281</th>\n",
       "      <td>PTCDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13285</th>\n",
       "      <td>PTEHG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13290</th>\n",
       "      <td>PTICU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6480 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_aeronave\n",
       "33          PRMYJ\n",
       "35          PRAKJ\n",
       "45          PTMXC\n",
       "47          PRWBF\n",
       "83          PRCHS\n",
       "...           ...\n",
       "13276       PTWKZ\n",
       "13279       PTVCH\n",
       "13281       PTCDB\n",
       "13285       PTEHG\n",
       "13290       PTICU\n",
       "\n",
       "[6480 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe.id_ocorrencia.unique())\n",
    "len(dataframe.id_ocorrencia.values)\n",
    "dataframe.loc[dataframe['id_aeronave'].duplicated(),['id_aeronave']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd793ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The 'id_ocorrencia' column should have unique values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:       id_ocorrencia\n",
      "22             87102\n",
      "63             87056\n",
      "177            86916\n",
      "748            86205\n",
      "753            86201\n",
      "...              ...\n",
      "11998          41609\n",
      "12396          38419\n",
      "12397          38419\n",
      "12398          38419\n",
      "12711          36326\n",
      "\n",
      "[116 rows x 1 columns]\n",
      "WARNING:root:The id_ocorrencia has duplicated values. Shouldn't they be unique?\n",
      "WARNING:root:Column 'id_aeronave' has missing values.\n",
      "WARNING:root:The id_aeronave has duplicated values. Shouldn't they be unique?\n",
      "WARNING:root:Column 'categoria_operador' has missing values.\n",
      "WARNING:root:Column 'tipo_veiculo' has missing values.\n",
      "WARNING:root:Column 'nome_fabricante' has missing values.\n",
      "WARNING:root:Column 'nome_modelo' has missing values.\n",
      "WARNING:root:Column 'tipo_icao' has missing values.\n",
      "WARNING:root:Column 'tipo_motor' has missing values.\n",
      "WARNING:root:Column 'quantidade_motores' has missing values.\n",
      "WARNING:root:Column 'pmd_aeronave' has missing values.\n",
      "WARNING:root:Column 'categoria_pmd' has missing values.\n",
      "WARNING:root:Column 'quantidade_assentos' has missing values.\n",
      "WARNING:root:Column 'ano_fabricacao' has missing values.\n",
      "WARNING:root:Column 'nome_pais_fabricante' has missing values.\n",
      "WARNING:root:Column 'nome_pais_registro' has missing values.\n",
      "WARNING:root:Column 'categoria_registro' has missing values.\n",
      "WARNING:root:Column 'segmento_registro' has missing values.\n",
      "WARNING:root:Column 'nome_voo_origem' has missing values.\n",
      "WARNING:root:Column 'nome_voo_destino' has missing values.\n",
      "WARNING:root:Column 'fase_operacao' has missing values.\n",
      "WARNING:root:Column 'tipo_operacao' has missing values.\n",
      "WARNING:root:Column 'nivel_dano' has missing values.\n",
      "WARNING:root:Column 'quantidade_fatalidades' has missing values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'id_ocorrencia' column should have unique values.\n",
      "       id_ocorrencia\n",
      "22             87102\n",
      "63             87056\n",
      "177            86916\n",
      "748            86205\n",
      "753            86201\n",
      "...              ...\n",
      "11998          41609\n",
      "12396          38419\n",
      "12397          38419\n",
      "12398          38419\n",
      "12711          36326\n",
      "\n",
      "[116 rows x 1 columns]\n",
      "Column 'id_aeronave' has missing values.\n",
      "Column 'categoria_operador' has missing values.\n",
      "Column 'tipo_veiculo' has missing values.\n",
      "Column 'nome_fabricante' has missing values.\n",
      "Column 'nome_modelo' has missing values.\n",
      "Column 'tipo_icao' has missing values.\n",
      "Column 'tipo_motor' has missing values.\n",
      "Column 'quantidade_motores' has missing values.\n",
      "Column 'pmd_aeronave' has missing values.\n",
      "Column 'categoria_pmd' has missing values.\n",
      "Column 'quantidade_assentos' has missing values.\n",
      "Column 'ano_fabricacao' has missing values.\n",
      "Column 'nome_pais_fabricante' has missing values.\n",
      "Column 'nome_pais_registro' has missing values.\n",
      "Column 'categoria_registro' has missing values.\n",
      "Column 'segmento_registro' has missing values.\n",
      "Column 'nome_voo_origem' has missing values.\n",
      "Column 'nome_voo_destino' has missing values.\n",
      "Column 'fase_operacao' has missing values.\n",
      "Column 'tipo_operacao' has missing values.\n",
      "Column 'nivel_dano' has missing values.\n",
      "Column 'quantidade_fatalidades' has missing values.\n"
     ]
    }
   ],
   "source": [
    "dataframe = df_aeronave\n",
    "# Check for unique values in the 'id_ocorrencia' column\n",
    "if not dataframe['id_ocorrencia'].is_unique:\n",
    "    logging.warning(\"The 'id_ocorrencia' column should have unique values.\")\n",
    "    print(\"The 'id_ocorrencia' column should have unique values.\")\n",
    "    logging.warning(dataframe.loc[dataframe['id_ocorrencia'].duplicated(),['id_ocorrencia']])\n",
    "    print(dataframe.loc[dataframe['id_ocorrencia'].duplicated(),['id_ocorrencia']])\n",
    "if not dataframe[dataframe.duplicated()].empty:\n",
    "    logging.warning(\"The dataframe has duplicated rows:\")\n",
    "    print(\"The dataframe has duplicated rows:\")\n",
    "    logging.warning(dataframe[dataframe.duplicated()])\n",
    "    print(dataframe[dataframe.duplicated()])\n",
    "# Check for missing values in the columns\n",
    "for col in dataframe.columns:\n",
    "    if dataframe[col].isnull().any():\n",
    "        logging.warning(f\"Column '{col}' has missing values.\")\n",
    "        print(f\"Column '{col}' has missing values.\")\n",
    "    if col.startswith('id'):\n",
    "        # Check for unique values in the 'id_relatorio' column\n",
    "        if dataframe[col].is_unique:\n",
    "            pass\n",
    "        else:\n",
    "            logging.warning(f\"The {col} has duplicated values. Shouldn't they be unique?\")\n",
    "# Check for duplicate rows\n",
    "if dataframe.duplicated().any():\n",
    "    logging.warning(\"There are duplicate rows in the DataFrame.\")\n",
    "    print(\"There are duplicate rows in the DataFrame.\")\n",
    "    logging.info(dataframe[dataframe.duplicated()])\n",
    "    print(dataframe[dataframe.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11726225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
